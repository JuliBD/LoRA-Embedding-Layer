{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shpae: torch.Size([256000, 2304])\n",
      "Tokenizer vocab: ['▁suppress', 'SCRIPTION', '▁mediodía', 'Mods', '▁bekerja', '▁dun', '▁secretos', '▁Aquila', 'dą', 'рева', '▁terrible', '▁Duchess', '▁anspruchs', '疼', '▁RAG', 'こそ', '▁Tallinn', 'Voltaje', '拼接', '▁overlap', 'од', 'genre', '海老', 'DESIGN', '▁Eigenschaften', '▁daar', '▁exec', 'mayacak', '▁достав', 'sto', 'ͦ', '팰', 'tableLayoutPanel', '▁received', '▁calibrate', ']);\\r', 'ListActivity', '▁نوم', '▁fita', 'canyon', '▁tsi', '▁ребенок', 'tershire', '產品', '▁forklar', '▁polis', '▁दिख', 'bricks', 'บบ', '▁ospiti']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "embed_layer = torch.load(\"gemma-2-2b_embedding-layer.pth\")[\"weight\"]\n",
    "print(\"Embedding shpae:\", embed_layer.shape)\n",
    "print(\"Tokenizer vocab sample:\", list(tokenizer.vocab)[:50])\n",
    "\n",
    "normalized_embed_layer = embed_layer / embed_layer.norm(dim=-1, keepdim=True)\n",
    "nld_embed_layer = normalized_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'Write', '▁me', '▁a', '▁poem', '▁about', '▁Machine', '▁Learning', '.']\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.7823, 0.7610, 0.7584, 0.6836, 0.6510, 0.6421, 0.6361, 0.6078,\n",
      "        0.6026, 0.5995, 0.5823, 0.5590, 0.5539, 0.5494, 0.5450, 0.5341, 0.5319,\n",
      "        0.5209, 0.5207, 0.5059, 0.5020, 0.4861, 0.4855, 0.4811, 0.4760, 0.4741,\n",
      "        0.4736, 0.4709, 0.4559], device='cuda:0')\n",
      "['▁poem']\n",
      "['▁poem', '▁Poem', '▁poems', 'poem', 'Poem', '▁poetry', '▁Poems', '▁poet', '▁poema', '▁Poetry', 'Poems', 'poetry', 'Poetry', '▁poets', '▁poème', '▁Poet', '▁Gedicht', '▁poemas', '▁POETRY', '▁poetic', '▁стихотво', 'poet', '▁poesia', 'Poet', '▁Poets', '▁Gedichte', '▁poesía', '▁POEMS', '▁poetical', '▁sonnet']\n"
     ]
    }
   ],
   "source": [
    "cur_id = input_ids[\"input_ids\"][0,4]\n",
    "cos_sim = nld_embed_layer @ nld_embed_layer[cur_id]\n",
    "cos_sim_sort = cos_sim.argsort(descending=True)\n",
    "k = 30\n",
    "top_k_ids = cos_sim_sort[:k]\n",
    "top_k_sim = cos_sim[top_k_ids]\n",
    "print(top_k_sim)\n",
    "print(tokenizer.convert_ids_to_tokens([cur_id]))\n",
    "print(tokenizer.convert_ids_to_tokens(top_k_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LoRA-Embedding-Layer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
